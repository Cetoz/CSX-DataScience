class(unlist(ltnnews[,4]))
head(unlist(ltnnews[,4]))
ltnnews[,4] <- unlist(ltnnews[,4])
head(ltnnews[,4])
write(ltnnews[,4],file = 'test.txt')
write(ltnnews[,4],file = 'test.txt',append = TRUE)
test <- ltnnews[,4][ltnnews[,5]]
test
test <- ltnnews[,4][ltnnews[,5]==29]
test
test <- ltnnews[,4][ltnnews[,5]==29|30]
length(test)
29==29|30
28==29|30
27==29|30
29|30
test <- ltnnews[,4][ltnnews[,5]==29|ltnnews[,5]==30]
length(test)
ltnnews[,4][ltnnews[,5]==29|ltnnews[,5]==30] %>%
write(file = '2930.txt')
ltnnews[,4][ltnnews[,5]==29|ltnnews[,5]==30] %>%
write(file = '2930.txt')
ltnnews[,4][ltnnews[,5]==31|ltnnews[,5]==01] %>%
write(file = '3101.txt')
ltnnews[,4][ltnnews[,5]==02|ltnnews[,5]==03] %>%
write(file = '0203.txt')
ltnnews[,4][ltnnews[,5]==04|ltnnews[,5]==05] %>%
write(file = '0405.txt')
ltnnews[,4][ltnnews[,5]==08|ltnnews[,5]==09] %>%
write(file = '0809.txt')
ltnnews[,4][ltnnews[,5]==06|ltnnews[,5]==07] %>%
write(file = '0607.txt')
ltnnews[,5]==04
class(ltnnews[,5])
ltnnews[,5]==31
ltnnews[,5]=='04'
ltnnews[,4][ltnnews[,5]=='29'|ltnnews[,5]=='30'] %>%
write(file = '2930.txt')
ltnnews[,4][ltnnews[,5]=='31'|ltnnews[,5]=='01'] %>%
write(file = '3101.txt')
ltnnews[,4][ltnnews[,5]=='02'|ltnnews[,5]=='03'] %>%
write(file = '0203.txt')
ltnnews[,4][ltnnews[,5]=='04'|ltnnews[,5]=='05'] %>%
write(file = '0405.txt')
ltnnews[,4][ltnnews[,5]=='06'|ltnnews[,5]=='07'] %>%
write(file = '0607.txt')
ltnnews[,4][ltnnews[,5]=='08'|ltnnews[,5]=='09'] %>%
write(file = '0809.txt')
library(jiebaR)
mixseg = worker()
?segment
?DirSource
?write
ltnnews[,4][ltnnews[,5]=='29'|ltnnews[,5]=='30'] %>%
write(file = '2930.txt')
ltnnews[,4][ltnnews[,5]=='31'|ltnnews[,5]=='01'] %>%
write(file = '3101.txt')
ltnnews[,4][ltnnews[,5]=='02'|ltnnews[,5]=='03'] %>%
write(file = '0203.txt')
ltnnews[,4][ltnnews[,5]=='04'|ltnnews[,5]=='05'] %>%
write(file = '0405.txt')
ltnnews[,4][ltnnews[,5]=='06'|ltnnews[,5]=='07'] %>%
write(file = '0607.txt')
ltnnews[,4][ltnnews[,5]=='08'|ltnnews[,5]=='09'] %>%
write(file = '0809.txt')
d.corpus <- Corpus( DirSource("C:\Users\FuHung\Documents\GitHub\CSX-DataScience\week_5") )
d.corpus <- Corpus( DirSource("C:\\Users\FuHung\Documents\GitHub\CSX-DataScience\week_5") )
d.corpus <- Corpus( DirSource("C:\Users\FuHung\Documents\GitHub\CSX-DataScience\week_5") )
?DirSource
d.corpus <- Corpus( DirSource("C:\\Users\\FuHung\\Documents\\GitHub\\CSX-DataScience\\week_5") )
library(tm)
d.corpus <- Corpus( DirSource("C:\\Users\\FuHung\\Documents\\GitHub\\CSX-DataScience\\week_5") )
d.corpus <- tm_map(d.corpus, removePunctuation)
d.corpus <- tm_map(d.corpus, removeNumbers)
d.corpus <- tm_map(d.corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)
})
mixseg = worker(stop_word = 'C:\\Users\\FuHung\\Documents\\GitHub\\CSX-DataScience\\week_5\StopWord\\stop_word.txt')
mixseg = worker(stop_word = 'C:\\Users\\FuHung\\Documents\\GitHub\\CSX-DataScience\\week_5\\StopWord\\stop_word.txt')
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(d.corpus, jieba_tokenizer)
heead(seg)
head(seg)
seg = lapply(d.corpus, jieba_tokenizer)
count_token = function(d)
{
as.data.frame(table(d))
}
tokens = lapply(seg, count_token)
head(tokens)
library(tidytext)
?bind_tf_idf
?unnest_tokens()
book_words <- austen_books() %>%
unnest_tokens(word, text) %>%
count(book, word, sort = TRUE) %>%
ungroup()
library(janeaustenr)
book_words <- austen_books() %>%
unnest_tokens(word, text) %>%
count(book, word, sort = TRUE) %>%
ungroup()
??count
library(dplyr)
book_words <- austen_books() %>%
unnest_tokens(word, text) %>%
count(book, word, sort = TRUE) %>%
ungroup()
book_words
book_words <- austen_books() %>%
unnest_tokens(word, text)
book_words
austen_books()
?unnest_tokens()
terms <-DocumentTermMatrix(corpus,control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))
terms <-DocumentTermMatrix(d.corpus,control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))
head(terms)
head(terms)
terms
?TermDocumentMatrix
terms <-TermDocumentMatrix(d.corpus,control = list(language = 'zh-TW',weighting = function(x) weightTfIdf(x, normalize = FALSE)))
terms
tokens
d.corpus <- Corpus( DirSource("C:\\Users\\FuHung\\Documents\\GitHub\\CSX-DataScience\\week_5") )
d.corpus <- tm_map(d.corpus, removePunctuation)
d.corpus <- tm_map(d.corpus, removeNumbers)
d.corpus <- tm_map(d.corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)
})
d.corpus
d.corpus <- Corpus( DirSource("C:\\Users\\FuHung\\Documents\\GitHub\\CSX-DataScience\\week_5") )
d.corpus <- tm_map(d.corpus, removePunctuation)
d.corpus <- tm_map(d.corpus, removeNumbers)
d.corpus <- tm_map(d.corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)
})
d.corpus
inspect(d.corpus)
inspect(d.corpus[7])
d.corpus <- Corpus( DirSource("C:\\Users\\FuHung\\Documents\\GitHub\\CSX-DataScience\\week_5\\data") )
d.corpus <- tm_map(d.corpus, removePunctuation)
d.corpus <- tm_map(d.corpus, removeNumbers)
d.corpus <- tm_map(d.corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)
})
d.corpus
mixseg = worker(stop_word = 'C:\\Users\\FuHung\\Documents\\GitHub\\CSX-DataScience\\week_5\\stop_word.txt')
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(d.corpus, jieba_tokenizer)
count_token = function(d)
{
as.data.frame(table(d))
}
tokens = lapply(seg, count_token)
head(tokens[1])
n = length(seg)
TDM = tokens[[1]]
colNames <- names(seg)
colNames <- gsub(".txt", "", colNames)
for( id in c(2:n) )
{
TDM = merge(TDM, tokens[[id]], by="d", all = TRUE)
names(TDM) = c('d', colNames[1:id])
}
head(seg)
tokens[[1]]
colNames <- names(seg)
?gsub
n
?merge
for( id in c(2:n) ){
TDM = merge(TDM, tokens[[id]], by="d", all = TRUE)
names(TDM) = c('d', colNames[1:id])
}
tokens[[2]]
TDM
?names
n = length(seg)
TDM = tokens[[1]]
colNames <- names(seg)
colNames <- gsub(".txt", "", colNames)
colNames
colNames
?gsub
colNames <- names(seg)
colNames
seg
names(seg)
d.corpus
library(bitops)
library(httr)
library(RCurl)
install.packages('RCurl')
library(RCurl)
library(XML)
install.packages('XML')
mixseg = worker(stop_word = 'C:\\Users\\FuHung\\Documents\\GitHub\\CSX-DataScience\\week_5\\stop_word.txt')
library(XML)
library(tm)
library(NLP)
library(tmcn)
install.packages('tmcn')
library(tmcn)
library(jiebaRD)
library(jiebaR)
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(d.corpus, jieba_tokenizer)
seg
d.corpus <- Corpus( DirSource("C:\\Users\\FuHung\\Documents\\GitHub\\CSX-DataScience\\week_5\\data") )
d.corpus
inspect(d.corpus[1])
d.corpus <- tm_map(d.corpus, removePunctuation)
d.corpus <- tm_map(d.corpus, removeNumbers)
d.corpus <- tm_map(d.corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)
})
mixseg = worker(stop_word = 'C:\\Users\\FuHung\\Documents\\GitHub\\CSX-DataScience\\week_5\\stop_word.txt')
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(d.corpus, jieba_tokenizer)
count_token = function(d)
{
as.data.frame(table(d))
}
tokens = lapply(seg, count_token)
seg
colNames <- c('0203','0405','0607','0809','2930','3101')
for( id in c(2:n) ){
TDM = merge(TDM, tokens[[id]], by="d", all = TRUE)
names(TDM) = c('d', colNames[1:id])
}
TDM
TDM[is.na(TDM)] <- 0
library(knitr)
kable(head(TDM))
?kable
kable(tail(TDM))
tf <- apply(as.matrix(TDM[,2:(n+1)]), 2, sum)
library(Matrix)
idfCal <- function(word_doc)
{
log2( n / nnzero(word_doc) )
}
idf <- apply(as.matrix(TDM[,2:(n+1)]), 1, idfCal)
doc.tfidf <- TDM
tempY = matrix(rep(c(as.matrix(tf)), each = length(idf)), nrow = length(idf))
tempX = matrix(rep(c(as.matrix(idf)), each = length(tf)), ncol = length(tf), byrow = TRUE)
doc.tfidf[,2:(n+1)] <- (doc.tfidf[,2:(n+1)] / tempY) * tempX
stopLine = rowSums(doc.tfidf[,2:(n+1)])
delID = which(stopLine == 0)
kable(head(doc.tfidf[delID,1]))
kable(tail(doc.tfidf[delID,1]))
TDM = TDM[-delID,]
doc.tfidf = doc.tfidf[-delID,]
TopWords = data.frame()
for( id in c(1:n) )
{
dayMax = order(doc.tfidf[,id+1], decreasing = TRUE)
showResult = t(as.data.frame(doc.tfidf[dayMax[1:5],1]))
TopWords = rbind(TopWords, showResult)
}
rownames(TopWords) = colnames(doc.tfidf)[2:(n+1)]
TopWords = droplevels(TopWords)
kable(TopWords)
TDM
d.corpus <- Corpus( DirSource("C:\\Users\\FuHung\\Documents\\GitHub\\CSX-DataScience\\week_5\\data") )
d.corpus <- tm_map(d.corpus, removePunctuation)
d.corpus <- tm_map(d.corpus, removeNumbers)
d.corpus <- tm_map(d.corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)
})
mixseg = worker(stop_word = 'C:\\Users\\FuHung\\Documents\\GitHub\\CSX-DataScience\\week_5\\stop_word.txt')
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(d.corpus, jieba_tokenizer)
count_token = function(d)
{
as.data.frame(table(d))
}
tokens = lapply(seg, count_token)
n = length(seg)
TDM = tokens[[1]]
colNames <- c('0203','0405','0607','0809','2930','3101')
for( id in c(2:n) ){
TDM = merge(TDM, tokens[[id]], by="d", all = TRUE)
names(TDM) = c('d', colNames[1:id])
}
TDM[is.na(TDM)] <- 0
kable(head(TDM))
TDM
TDM[,3]
TDM <- TDM[,c(5,6,1,2,3,4)]
kable(head(TDM))
d.corpus <- Corpus( DirSource("C:\\Users\\FuHung\\Documents\\GitHub\\CSX-DataScience\\week_5\\data") )
d.corpus <- tm_map(d.corpus, removePunctuation)
d.corpus <- tm_map(d.corpus, removeNumbers)
d.corpus <- tm_map(d.corpus, function(word) {
gsub("[A-Za-z0-9]", "", word)
})
mixseg = worker(stop_word = 'C:\\Users\\FuHung\\Documents\\GitHub\\CSX-DataScience\\week_5\\stop_word.txt')
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(d.corpus, jieba_tokenizer)
count_token = function(d)
{
as.data.frame(table(d))
}
tokens = lapply(seg, count_token)
n = length(seg)
TDM = tokens[[1]]
colNames <- c('0203','0405','0607','0809','2930','3101')
for( id in c(2:n) ){
TDM = merge(TDM, tokens[[id]], by="d", all = TRUE)
names(TDM) = c('d', colNames[1:id])
}
TDM[is.na(TDM)] <- 0
TDM <- TDM[,c(1,6,7,2,3,4,5)]
kable(head(TDM))
kable(tail(TDM))
tf <- apply(as.matrix(TDM[,2:(n+1)]), 2, sum)
idfCal <- function(word_doc)
{
log2( n / nnzero(word_doc) )
}
idf <- apply(as.matrix(TDM[,2:(n+1)]), 1, idfCal)
doc.tfidf <- TDM
tempY = matrix(rep(c(as.matrix(tf)), each = length(idf)), nrow = length(idf))
tempX = matrix(rep(c(as.matrix(idf)), each = length(tf)), ncol = length(tf), byrow = TRUE)
doc.tfidf[,2:(n+1)] <- (doc.tfidf[,2:(n+1)] / tempY) * tempX
stopLine = rowSums(doc.tfidf[,2:(n+1)])
delID = which(stopLine == 0)
kable(head(doc.tfidf[delID,1]))
kable(tail(doc.tfidf[delID,1]))
TDM = TDM[-delID,]
doc.tfidf = doc.tfidf[-delID,]
TopWords = data.frame()
for( id in c(1:n) )
{
dayMax = order(doc.tfidf[,id+1], decreasing = TRUE)
showResult = t(as.data.frame(doc.tfidf[dayMax[1:5],1]))
TopWords = rbind(TopWords, showResult)
}
rownames(TopWords) = colnames(doc.tfidf)[2:(n+1)]
TopWords = droplevels(TopWords)
kable(TopWords)
TDM
?file.size
setwd("~/GitHub/CSX-DataScience/week_5/data")
file.size('0203.txt')
file.size('0203.txt')/1024
file.size('.txt')
apply(filename,file.size)
?apply
apply(filename,1,file.size)
filename <- c('2930.txt','3101.txt','0203.txt','0405.txt','0607.txt','0809.txt')
apply(filename,1,file.size)
?mapply
?tapply
?mapply
mapply(rep, 1:4, 4:1)
4:1
apply(filename,1,file.size)
filename <- data.frame('2930.txt','3101.txt','0203.txt','0405.txt','0607.txt','0809.txt')
apply(filename,1,file.size)
apply(filename,1,file.size/1024)
filesize <- apply(filename,1,file.size)/1024
filesize
library(ggplot2)
library(ggplot)
install.packages('ggplot2')
library(ggplot2)
filesizeplot <- ggplot(aes(x=filesize))
filesize
filesizeplot <- ggplot(aes(x=filesize[,1]))
filesizeplot <- ggplot(data=filesize,aes(x=filesize[,1]))
filesizeplot <- ggplot(data=filesize,aes(x=[,1]))
filesize
class(filesize)
filesize <- as.dataframe(apply(filename,1,file.size)/1024)
filesize <- as.data.frame(apply(filename,1,file.size)/1024)
filesizeplot <- ggplot(data=filesize,aes(x=[,1]))
filesizeplot <- ggplot(data=filesize,aes(x=datasize[,1]))
filesizeplot <- ggplot(data=filesize,aes(x=datasize[,1]))+geom_bar()
filesizeplot
filesizeplot <- ggplot(data=filesize,aes(x=filesize[,1]))+geom_bar()
filesizeplot
filesizeplot <- ggplot(data=filesize,aes(x=filesize[,1]))+geom_bar(stat = 'identity')
filesizeplot
filesize
daytag <- c('2930','3101','0203','0405','0607','0809')
filesize <- cbind(daytag,filesize)
filesize
filename <- data.frame('2930.txt','3101.txt','0203.txt','0405.txt','0607.txt','0809.txt')
filesize <- as.data.frame(apply(filename,1,file.size)/1024)
daytag <- c('29-30','31-01','02-03','04-05','06-07','08-09')
filesize <- cbind(daytag,filesize)
filesizeplot <- ggplot(data=filesize,aes(x=daytag,y=filesize[,1]))+geom_bar(stat = 'identity')
filesizeplot
filesizeplot <- ggplot(data=filesize,aes(x=daytag,y=filesize[,1]))+geom_bar()
filesizeplot
filesizeplot <- ggplot(data=filesize,aes(x=daytag,y=filesize[,1]))+geom_histogram()
gilesizeplot
filesizeplot
filesize[,1]
filesizeplot <- ggplot(data=filesize,aes(x=daytag,y=filesize[,2]))+geom_histogram()
filesize[,1]
filesizeplot
filesizeplot <- ggplot(data=filesize,aes(x=daytag,y=filesize[,2]))+geom_bar(stat = 'identity')
filesizeplot
?geom_bar
filesizeplot <- ggplot(data=filesize,aes(x=daytag,y=filesize[,2])) +
geom_bar(stat = 'identity') +
xlab('Day')+
ylab('size_KB')+
scale_x_discrete(limits = daytag)
filesizeplot
TopWords
TDM
DTM[3]
TDM[3]
desc(TDM[3])
page <- c(1:25)
urllist=list()
for(i in page){
url='http://news.ltn.com.tw/search/?keyword=孫安佐&conditions=and&SYear=2018&SMonth=01&SDay=09&EYear=2018&EMonth=04&EDay=09&page='
pasteurl <- paste0(url,i)
urllist <- rbind(urllist,pasteurl)
}
ltnnews <- matrix()
for(i in page){
docs <- read_html(urllist[[i]])
title <- html_nodes(docs,'.tit p') %>%
html_text(title)
link <- html_nodes(docs,'a.tit') %>%
html_attr('href') %>%
paste0('http://news.ltn.com.tw/',.)
titleandlink <- cbind(title,link)
ltnnews <-rbind(ltnnews,titleandlink)
}
page <- c(1:25)
urllist=list()
for(i in page){
url='http://news.ltn.com.tw/search/?keyword=孫安佐&conditions=and&SYear=2018&SMonth=01&SDay=09&EYear=2018&EMonth=04&EDay=09&page='
pasteurl <- paste0(url,i)
urllist <- rbind(urllist,pasteurl)
}
ltnnews <- list()
for(i in page){
docs <- read_html(urllist[[i]])
title <- html_nodes(docs,'.tit p') %>%
html_text(title)
link <- html_nodes(docs,'a.tit') %>%
html_attr('href') %>%
paste0('http://news.ltn.com.tw/',.)
titleandlink <- cbind(title,link)
ltnnews <-rbind(ltnnews,titleandlink)
}
page <- c(1:25)
urllist=list()
for(i in page){
url='http://news.ltn.com.tw/search/?keyword=孫安佐&conditions=and&SYear=2018&SMonth=01&SDay=09&EYear=2018&EMonth=04&EDay=09&page='
pasteurl <- paste0(url,i)
urllist <- rbind(urllist,pasteurl)
}
ltnnews <- list()
for(i in page){
docs <- read_html(urllist[[i]])
title <- html_nodes(docs,'.tit p') %>%
html_text(title)
link <- html_nodes(docs,'a.tit') %>%
html_attr('href') %>%
paste0('http://news.ltn.com.tw/',.)
titleandlink <- cbind(title,link)
ltnnews <-rbind(ltnnews,titleandlink)
}
View(ltnnews)
class(ltnnews)
class(ltnnews[1])
class(ltnnews[2])
class(ltnnews[3])
class(ltnnews[1])
class(ltnnews[[1]])
length(ltnnews)
test <- as.data.frame(ltnnews[[1:length(ltnnews)]])
test <- as.character(ltnnews[[c(1:length(ltnnews))]])
test <- as.character(ltnnews[c(1:length(ltnnews))])
View(test)
head(test)
View(test)
View(test)
test <- unlist(ltnnews[c(1:length(ltnnews))])
test
class(ltnnews[700])
test[2,2]
ltnnews[1,2]
ltnnews[[1,2]]
?Corpus
